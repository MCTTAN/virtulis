---
title: "missionboard"
output:
  html_document: default
  pdf_document: default
---

```{r}
# Overview
#
# - Divide the iris dataset into training data and
#   testing data.
# - Create a model after training C5.0 to predict
#   the species using the training data.
# - Test your model with the testing data.
# - Evaluate your model using a confusion matrix.
#
# - Use k-means to group "similar" data.
# - Test the k-means clusters.
#
# - Train Support Vector Machines (SVMs) to recognize
#   3 different species of irises.
# - Test the SVMs to see how accurately it learned
#   the different species.
#
# - Use apropri.
#
# - Use Expectation Maximization (EM).
#
# - Use the PageRank algorithm to determine the relative
#   importance of objects in a graph.
```

```{r}
# https://hackerbits.com/data/top-10-data-mining-algorithms-in-plain-r/
```

```{r}
library(adabag)
library(arules)
library(C50)
library(dplyr)
library(e1071)
library(igraph)
library(mclust)
library(printr)
library(stats)
```

```{r}
# The iris dataset has 5 columns:
# - Sepal.Length
# - Sepal.Width
# - Petal.Length
# - Petal.Width
# - Species

# Each row is numbered 1 through 150.

# Divide the data into training data and test data.
```

```{r}
# Take a sample of 100 rows from the iris dataset.

# Take a random 100 row sample from 1 through 150.
train.indeces <- sample(1:nrow(iris), 100)

iris.train <- iris[train.indeces, ]

# Select some rows and all columns
# other than the 100 you already sampled
iris.test <- iris[-train.indeces, ]
```

```{r}
# Train a model based on the training data.
# The Species column depends on the other columns
# (Sepal.Width, Petal.Height, etc.). The tilde means
# "depends", and the period means "all the other columns".
# You're essentially writing code for "Species depends on
# all the other column data".

model <- C5.0(Species ~ ., data = iris.train)
```

```{r}
# Test the C5.0 model using cross-validation.

# predict() takes your model (the test data) and one
# parameter (the model indicate species) that tells it to
# guess the class. It then attempts to predict the species
# based on the other data columns, and stores the results
# in the variable results.

results <- predict(object = model, newdata = iris.test, type = "class")
```

```{r}
# Check the results using a confusion matrix
# (a.k.a. a contingency table).

# A confusion matrix allows us to visually compare the
# predicted species vs. the actual species.

# The rows represent the predicted species, and the columns represent the actual species from the iris dataset.
# 
# Starting from the setosa row, you would read this as:
# 
# 21 iris observations were predicted to be setosa when they were actually setosa.
# 14 iris observations were predicted to be versicolor when they were actually versicolor.
# 1 iris observation was predicted to be versicolor when it was actually virginica.
# 14 iris observations were predicted to be virginica when it was actually virginica.

table(results, iris.test$Species)
```

```{r}
########################################################
```

```{r}
# k-means is a cluster analysis technique used for
# forming groups around data that "look similar".
# The problem k-means solves is: We don't know which
# data belongs to which group - we don't even know
# the number of groups, but k-means can help.

# This code removes the Species column from the iris
# dataset and uses k-means to create 3 clusters.

# subset() is used to remove the Species column from the
# iris dataset. It's no fun if we know the Species
# before clustering.

# kmeans() is applied to the iris dataset (with Species
# removed), and we tell it to create 3 clusters.

model <- kmeans(x = subset(iris, select = -Species), centers = 3)
```

```{r}
# Test the k-means clusters with the confusion matrix
# and the known species data.

# The numbers along the side are the cluster numbers.

# What does the matrix tell us?
# Here are some potential interpretations for the
# matrix:
# - k-means picked up really well on the characteristics
#   for setosa in cluster 2. Out of 50 setosa irises,
#   k-means grouped together all 50.
# - k-means had a tough time with versicolor and virginica,
#   since they are being grouped into both clusters 1 and 
#   2. Cluster 1 favors versicolor and cluster 3 strongly 
#   favors virginica.
# - An interesting investigation would be to try clustering #   the data into 2 clusters rather than 3. You could 
#   easily experiment with the centers parameter in 
#   kmeans() to see if that would work better.

table(model$cluster, iris$Species)
```

```{r}
# k-means didn't do great in this instance.
# Unfortunately, no algorithm will be able to cluster
# or classify in every case.

# Using this iris dataset, k-means could be used to
# cluster setosa and possibly virginica.
# With data minind, model testing/validation is super
# important, but we're not going to be able to cover
# it in this post.
```

```{r}
########################################################
```

```{r}
train.indeces <- sample(1:nrow(iris), 100)
iris.train <- iris[train.indeces, ]
iris.test <- iris[-train.indeces, ]
```

```{r}
# Train a model based on the training data.

# Species depends on the other columns.

# The data to train SVM is stored in iris.train.

model <- svm(Species ~ ., data = iris.train)
```

```{r}
# Test the model using the test data.

results <- predict(object = model, newdata = iris.test, type = "class")
```

```{r}
# Generate a confusion matrix for the results.

table(results, iris.test$Species)
```

```{r}
# SVM and C5.0 seem to do about the same on this
# dataset, and both do better than k-means.
```

```{r}
########################################################
```

```{r}
data("Adult")
```

```{r}
# apriori() stores the association rules into rules.

# apriori() filters the generated rules.

# apriori() looks for certain characteristics in the
# association rules.

# Association rules look like this:
# {United States} => {White, Male}
# ("When I see United States, I will also see White, 
# Male.")
# (We want to see race=White and sex=Male on the right-
# hand side. The left-hand side can remain the default.)

# support is the percentage of records in the dataset
# that contain the related items. Here, you're saying
# that you want at least 40% support.

# Confidence is the conditional probability of some item
# given that you have certain other items in your
# itemset.

rules <- apriori(Adult, parameter = list(support = 0.4, confidence = 0.7), appearance = list(rhs = c("race=White", "sex=Male"), default = "lhs"))
```

```{r}
# View the rules

# Sort the rules by lift.
# Lift tells you how strongly associated the left-hand
# and right-hand sides are associated with each other.
# The higher the lift value, the stronger the association.

# Grab the top 5 rules based on lift.

# Take the top 5 rules and convert them into a data frame.

# In the 1st ruleâ€¦ When we see Husband we are virtually guaranteed to see Male. Nothing surprising with this revelation. What would be interesting is to understand why itâ€™s not 100%. ðŸ™‚
# In the 2nd ruleâ€¦ Itâ€™s basically the same as the 1st rule, except weâ€™re dealing with civilian spouses. No surprises here.
# In the 3rd and 4th rulesâ€¦ When we see civilian spouse, we have a high chance of seeing Male and White. This is interesting, because it potentially tells us something about the data. Why isnâ€™t a similar rule for Female showing up? Why arenâ€™t rules for other races showing up in the top rules?
# In the 5th ruleâ€¦ When we see US, we tend to see White. This seems to fit with my expectation, but it could also point to the way the data was collected. Did we expect the data set to have more race=White?

# CAUTION: You may get rules that mislead you.
  
rules.sorted <- sort(rules, by = "lift")
top5.rules <- head(rules.sorted, 5)
as(top5.rules, "data.frame")
```

```{r}
########################################################
```

```{r}
# Expectation Maximization (EM) is used for
# clustering (like k-means).

# Cluster the irises using the EM algorithm.

# this code removes the Species column from the iris
# data set and uses Mclust() to create clusters.

# Mxlust() uses the EM algorithm under the hood.
# In a nutshell, Mclust() tunes a set of models using EM
# and then selects the one with the lowest BIC.

# Bayesian Informaiton Criterion (BIC), in a nutshell,
# given a efw models, is an index which measures both the explanatory power and the simplicity of a model.
# The simpler the model and the more data it can explain,
# the lower the BIC. The model with the lowest BIX is
# the winner.

model <- Mclust(subset(iris, select = -Species))
```

```{r}
# Generate a confusion matrix for the results.

# Just like k-means clustering, the clusters are numbered
# accordingly.

# There are only 2 clusters because Mclust() effectively
# segmented setosa from the other 2 species.

# Like k-means, EM had trouble distinguishing between
# versicolor and virginica. While k-means had some success
# with virginica and to a lesser degree versicolor,
# k-means made the effort to form a 3rd cluster because
# we told it to form 3 clusters.

table(model$classification, iris$Species)
```

```{r}
# Investigate the clustering plots.

plot(model)
```



